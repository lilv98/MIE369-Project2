//////////////////////////////////////////////////////////////////////
//
// First-Order Logic Package
//
// Class:  ClauseKb
// Author: Scott Sanner (ssanner@cs.toronto.edu)
// Date:   7/25/03
//
// NOTES:
// ------
// - Have managed to avoid redundant clauses by using a lexical ordering
//   for ConnNodes (see ReorderPNodes) and by always resetting variables
//   to start at 1 when standardizing apart for unification and when
//   generating the resulting clauses (reset counter to 1).  Seems to
//   catch the majority of redundant clauses.  Also introduced
//   standardizeApartClause to pay more attention to getting substitutions
//   in increasing variable id order from left to right of clauses.
//
// TODO:
// -----
// - Lots of room for efficiency improvement.  Too much String processing...
//   at least intern them and use object equality!
// - Remove error checks.
// - Original recursive Knuth-Bendix???  Lexicographic path ordering???
// - Redundancy criteria, forward/backward subsumption
// - Factoring with theory implication (retain most general)
// - Age/weight queues (good weighting heuristics)
// - Limited resource strategy
// - Expore ordering more (bottom to top in taxonomy yields huge
//   improvement): 1) taxonomy based topological sort (currently just
//   based on clause connectivity, and 2) non-DL, then roles, then 
//   concepts. 
// - Not currently handling *residue* so CONJ and ONE-OF concepts are
//   also converted to FOL so that instances are properly inferred.
//   Remove this?  What are implications for DISJ and COMPL concepts?
// - Bindings?
// - Proof output formats... TEX, GraphViz?
// - Cannot do positive ordered resolution -- theory resolution can
//   mask the negations leading to incompleteness.
// - Convert quantifier types when reading in.
// - SOS ordering requires all negative literals to be maximal???
//   So only literal selection in positive satellites?
// - SOS still generates a lot of resolutions due to ease of ordering
//   restrictions... looks like need to include hyperresolution as well,
//   from Otter:
//      This is a non-Horn set without equality.  The strategy will
//      be ordered hyper_res, unit deletion, and factoring, with
//      satellites in sos and with nuclei in usable.
//
//////////////////////////////////////////////////////////////////////

package logic.kb.fol;

import java.io.*;
import java.util.*;

import graph.*;
import logic.kb.*;
import logic.kb.fol.parser.*;
import ont.*;
import ont.io.*;

// This was clause kb which has a working incompleted ordered resolution reasoner!
public class DLFOLClauseKb extends Kb {

	// Print resolutions as they are made?
	public static final boolean PRINT_RESOLUTIONS = false;
	public static final boolean PRINT_RES_STEP = false;
	public static final boolean PRINT_ADDED_FORMULA = false;
	public static final boolean PRINT_MAX_RES = false;
	public static final boolean PRINT_CNF = false;
	public static boolean GEN_GRAPH = false;
	public static final boolean COLLECT_ONLY_CONSTANTS = false;
	public static final boolean MAINTAIN_PROOFS = true; // true;
	public static final boolean PRINT_PROOFS = true; // true;
	public static final boolean PRINT_FIRST_PROOF_ONLY = true; // true;
	public static final String QUERY = "QUERY";
	public static final int AGE_SELECT = 1;
	public static final int WEIGHT_SELECT = 5;

	public static final boolean USE_SOS = false;
	
	// For the timer
	public long _lTime;
	public boolean _bPrintDebug = false;
	public HashMap _hmProofs; // Tracks all derivations
	public HashMap _hmPred2Clause; // Pred is a String (name_arity)
	public HashMap _hmPred2Name; // Maps name_arity -> name
	public HashMap _hmPred2Arity; // Maps name_arity -> arity
	public HashSet _hsNonDLPred;  // Set of non-DL preds
	public List _lElimOrdering; // Ordering of predicates (as Strings:

	// name_arity)
	public Graph _graphClauses; // A graph of connectivity b/w predicates due to

	// clauses
	public BindingSet _bs; // Store query results
	public boolean ALL_PROOFS; // Retrieve one proof or all proofs (for getting

	// query bindings)
	public int MAX_ITERATIONS; // In case of infinite resolvents
	public ArrayList _alQueryBindings;
	public int _nResSteps;
	public int _nProofSteps;
	public int _nClausesGen;
	public int _nFirstQueryClause;
	public float _fQueryTime;
	public String _sQueryFile;
	public HashMap _hmPredIndex; // Hashes pred name to ordering index
	public HashSet _hsResolved; // Cache of resolved clause pairs
	public HashSet _hsFactored;
	public DLFOLKb _kb;
	public ArrayList _alClauses;
	public ClausePriorityQueue _llPassive;
	public HashSet _hsActive;
	public LinkedList _llSOS;
	public HashSet _hsUsable;

	public DLFOLClauseKb(DLFOLKb kb) {
		this(kb, 2500); // XXX
	}

	public DLFOLClauseKb(DLFOLKb kb, int max_res) {
		_kb = kb;
		MAX_ITERATIONS = max_res;

		_hmPred2Clause = new HashMap();
		_hmPred2Name = new HashMap();
		_hmPred2Arity = new HashMap();
		_hmPredIndex = new HashMap();
		_hsNonDLPred = new HashSet();
		_hmProofs = null;
		_lElimOrdering = null;
		_graphClauses = null;
		_bs = null;
		_alQueryBindings = null;
		ALL_PROOFS = false;
		_sQueryFile = null;

		_alClauses = new ArrayList();
	}

	// ///////////////////////////////////////////////////////////////////////////
	// Kb Interface
	// ///////////////////////////////////////////////////////////////////////////

	public void setQueryFile(String file) {
		_sQueryFile = file;
	}

	public float getQueryTime() {
		return _fQueryTime;
	}

	public int getNumInfClauses() {
		return _nClausesGen;
	}

	public int getProofLength() {
		return _nProofSteps;
	}

	// Convert a first-order formula to clauses and add to kb
	public void addFOPCFormula(String s) {
		addFOPCFormula(FOPC.parse(s));
	}

	public void addFOPCFormula(FOPC.Node n) {
		addFormula(n);
	}

	// Query to see if kb entails s (false only implies could not be proven)
	public boolean queryFOPC(String query) {
		return queryFOPC(FOPC.parse(query));
	}

	public boolean queryFOPC(FOPC.Node n) {
		ALL_PROOFS = false;      // Exit after first proof
		_alQueryBindings = null; // Prevent QUERY from being added to order
		if (MAINTAIN_PROOFS) {
			_hmProofs = new HashMap();
		}
		FOPC.Node temp_query = n.copy().convertNNF(true);
		_nFirstQueryClause = _alClauses.size();
		addFormula(temp_query);

		boolean ref_found = canRefute();

		// Print proofs?  (Call with display=false to just retrieve proof depth)
		if (ref_found) {
			printProofs(new FOPC.TNode(false), PRINT_PROOFS);
		}

		// Restore pre-query clause list
		for (int i = _alClauses.size() - 1; i >= _nFirstQueryClause; i--) {
			_alClauses.remove(i);
		}

		return ref_found;
	}

	public boolean queryFOPC(String assume, String query) {
		return queryFOPC(FOPC.parse(assume), FOPC.parse(query));
	}

	public boolean queryFOPC(FOPC.Node assume, FOPC.Node query) {
		FOPC.ConnNode imp = new FOPC.ConnNode(FOPC.ConnNode.IMPLY, assume,
				query);
		return queryFOPC(imp);
	}

	public BindingSet queryFOPCBindings(String query) {
		return queryFOPCBindings(FOPC.parse(query));
	}

	// Query for bindings of free vars
	public BindingSet queryFOPCBindings(FOPC.Node query) {

		// Set up the binding set, query implication, query node, binding vars
		ALL_PROOFS = true; // Retrieve all bindings
		if (MAINTAIN_PROOFS) {
			_hmProofs = new HashMap();
		}
		_bs = new BindingSet();
		query.setFreeVars();
		Set free_vars = query._hsFreeVarsOut;
		_hmPred2Arity.put(QUERY, new Integer(free_vars.size()));
		FOPC.PNode query_node = new FOPC.PNode(false, QUERY);
		Iterator i = free_vars.iterator();
		_alQueryBindings = new ArrayList();
		while (i.hasNext()) {
			FOPC.TVar qvar = (FOPC.TVar) i.next();
			query_node.addBinding(qvar);
			_alQueryBindings.add(qvar);
			_bs.addVar(qvar.toString());
		}
		FOPC.Node imp_node = new FOPC.ConnNode(FOPC.ConnNode.IMPLY, query,
				query_node);
		FOPC.Node query_neg = query_node.copy().convertNNF(true);
		// System.out.println("Query imp: " + imp_node.toFOLString());
		// System.out.println("Query neg: " + query_neg.toFOLString());

		// Now add the additional clauses and complete the proof
		_nFirstQueryClause = _alClauses.size();
		addFormula(imp_node);
		addFormula(query_neg);
		canRefute();
		ALL_PROOFS = false;

		// Print proofs?  (Call with display=false to just retrieve proof depth)
		printProofs(new FOPC.TNode(false), PRINT_PROOFS);

		// Restore pre-query clause list and other structures
		for (int j = _alClauses.size() - 1; j >= _nFirstQueryClause; j--) {
			_alClauses.remove(j);
		}
		_hmPred2Arity.remove(QUERY);

		return _bs.seal();
	}

	public BindingSet queryFOPCBindings(String assume, String query) {
		return queryFOPCBindings(FOPC.parse(assume), FOPC.parse(query));
	}

	public BindingSet queryFOPCBindings(FOPC.Node assume, FOPC.Node query) {
		FOPC.ConnNode imp = new FOPC.ConnNode(FOPC.ConnNode.IMPLY, assume, query);
		return queryFOPCBindings(imp);
	}

	// ///////////////////////////////////////////////////////////////////////////

	public void setOrdering() {
		setOrdering("tp.dot");
	}

	public void setOrdering(String filename) {

		// First, build the connectivity graph
		_graphClauses = new Graph(false);

		// Go through each set of clauses
		Set factors = new HashSet();
		HashSet pred_names = new HashSet();
		Iterator i = _alClauses.iterator();
		while (i.hasNext()) {

			ArrayList al = new ArrayList();
			FOPC.ConnNode c = (FOPC.ConnNode) i.next();
			Iterator k = c._alSubNodes.iterator();
			while (k.hasNext()) {
				FOPC.PNode p = (FOPC.PNode) k.next();
				String pred_rep = p._sPredName + "_" + p._nArity;
				al.add(pred_rep);
				pred_names.add(pred_rep); // Unused?
				
				// Check to see if relation subsumption
				if (p._sPredName == null) {
					_hsNonDLPred.add(pred_rep);
				} else {
					URI u = new URI(_kb._ir._context, p._sPredName);
	
					// If u1 => u2, then <u1, ~u2> must be disjoint.
					if (!_kb._tax._mmParents.keySet().contains(u)
						&& !_kb._tax._mmRParents.keySet().contains(u)) {
						_hsNonDLPred.add(pred_rep);
					}
				}
			}

			// System.out.println("Adding all pair links: " + al);
			_graphClauses.addAllPairLinks(al);
			factors.add(new HashSet(al));
		}

		// Now, get a low width ordering
		if (GEN_GRAPH) {
			_graphClauses.genDotFile(filename);
		}
		// _lElimOrdering = _graphClauses.greedyTWSort(new
		// ArrayList(pred_names), factors);

		// This gives a bottom to top ordering which seems to be most
		// efficient... reverse takes 100X as long.
		_lElimOrdering = _graphClauses.topologicalSort(true);
		if (_alQueryBindings != null) {
			String query_str = QUERY + "_" + _alQueryBindings.size();
			_lElimOrdering.remove(query_str);
			_lElimOrdering.add(query_str);
		}

		// Direct hash to determine order of predicate.
		// NOTE: EARLIER IN THE ORDER IS MORE MAXIMAL
		for (int index = 0; index < _lElimOrdering.size(); index++) {
			_hmPredIndex.put(_lElimOrdering.get(index), new Integer(index));
		}

		System.out.println("Ordering: " + _lElimOrdering);
		// System.out.println("Binary Tree Width: " +
		// _graphClauses._df.format(_graphClauses._dMaxBinaryWidth));
	}

	public boolean containsNegativeResidue(FOPC.ConnNode c, int pos) {
		for (int i = 0; i < c._alSubNodes.size(); i++) {
			if (i == pos) continue;
			FOPC.PNode p = (FOPC.PNode)c._alSubNodes.get(i);
			if (!p._bIsNegated) continue;
			String pred_name = p._sPredName + "_" + p._nArity;
			if (!_hsNonDLPred.contains(pred_name)) continue;
			
			// Residue, negated, non-DL predicate
			return true;
		}
		return false;
	}
	
	// INTERPRETATION OF RESULTS (s > t means s more maximal than t,
	// which means s occurs earlier in order):
	// 2 s > t
	// -2 s < t
	// 1 s >= t (s could be more maximal)
	// -1 s <= t
	// 0 s == t
	public int compareLiterals(FOPC.PNode s, FOPC.PNode t) {
		int s_index = ((Integer) _hmPredIndex.get(s._sPredName + "_"
				+ s._nArity)).intValue();
		int t_index = ((Integer) _hmPredIndex.get(t._sPredName + "_"
				+ t._nArity)).intValue();

		if (s_index < t_index) {
			return 2;
		} else if (t_index < s_index) {
			return -2;
		} else {

			// Here we lexicographically order the terms
			// using the non-recursive Knuth-Bendix technique of Vampire.
			ArrayList s_terms = new ArrayList();
			ArrayList t_terms = new ArrayList();
			for (int i = 0; i < s._alTermBinding.size(); i++) {
				s_terms.addAll(((FOPC.Term) s._alTermBinding.get(i))
						.getListOfTerms());
			}
			for (int i = 0; i < t._alTermBinding.size(); i++) {
				t_terms.addAll(((FOPC.Term) t._alTermBinding.get(i))
						.getListOfTerms());
			}

			if (s_terms.size() < t_terms.size()) {
				return 2;
			} else if (t_terms.size() < s_terms.size()) {
				return -2;
			}

			// Equal length so do lexicographic comparison
			String str_s = null, str_t = null;
			for (int i = 0; i < s._alTermBinding.size(); i++) {
				str_s = (String) s_terms.get(0);
				str_t = (String) t_terms.get(0);
				int comp = str_s.compareTo(str_t);
				if (comp < 0) {
					return 1;
				} else if (comp > 0) {
					return -1;
				}
			}
			return 0;
		}
	}

	// Returns true if kb inconsistent (will modify clauses so should be a
	// copy!)
	public boolean canRefute() {
		if (USE_SOS) {
			return canRefuteSOS();
		} else {
			return canRefuteNormal();
		}
	}
	
	public boolean canRefuteNormal() {

		_hsResolved = new HashSet();
		_hsFactored = new HashSet();
		_llPassive = new ClausePriorityQueue();
		_hsActive = new HashSet();
		_nResSteps = 0;
		_nClausesGen = 0;

		// First determine an elimination order (important if new predicates
		// added)
		setOrdering();

		// Initialize the active and passive lists
		// System.out.println("\n\n");
		//System.out.println("START: " + _alClauses);
		for (int i = 0; i < _alClauses.size(); i++) {
			FOPC.ConnNode c = (FOPC.ConnNode) _alClauses.get(i);
			if (isTautology(c)) {
				System.out.println("DISCARDING TAUTOLOGY: " + c);
				continue;
			}
			Clause cl = new Clause(c);
			// DEBUG
			// System.out.println("Adding clause: " + cl);
			
			//_llPassive.addLast(cl);
			_llPassive.insert(cl);
		}

		ResetTimer();
		Clause cur = null;

		// Take from passive first, then from active
		// Later should add a method for using age-weight ratio
		// to select active clause from age and weight queues!
		boolean found_refutation = false;

		// TODO: Make age and weight queues for selection.
		// System.out.println("\n\n");
		int iterations = 0;
		while (++iterations <= MAX_ITERATIONS && !_llPassive.isEmpty()
				&& (cur = (Clause) _llPassive.removeFirst()) != null) {

			if (PRINT_RES_STEP) {
				System.out.print("\n" + iterations 
						+ " A:" + _hsActive.size() + " P:" + _llPassive.size() 
						+ ", examining clause: " + cur);
			} else if (iterations % 100 == 0) {
				System.out.println(iterations 
						+ " A:" + _hsActive.size() + " P:" + _llPassive.size());
			}

			// Iterate through all active clauses, keeping track of resolvents
			Iterator i2 = _hsActive.iterator(); // XXX ***
			Set resolvents = factorClause(cur);
			if (PRINT_RES_STEP) {
				System.out.println(", factors: " + resolvents);
			}
			HashSet new_inferred = new HashSet();

			while (iterations < MAX_ITERATIONS && i2.hasNext()) {
				Clause active = (Clause) i2.next();
				_bPrintDebug = false;

				// DEBUG
				// if (cur.toString().indexOf("#R3") >= 0) {
				// System.out.println(" ++ resolve with clause: " + active);
				// _bPrintDebug = true;
				// }

				// Have cur and active, resolve and factor where possible
				Set res_set = resolveClauses(cur, active);
				// DEBUG
				if (PRINT_RES_STEP && !res_set.isEmpty()) {
					System.out.println("\n" + cur + "     " + active);
					System.out.println("------------------------------");
					Iterator i3 = res_set.iterator();
					while (i3.hasNext()) {
						System.out.println(((FOPC.Node) i3.next())
								.toFOLString()
								+ "\n");
					}
				}
				resolvents.addAll(res_set);

				Iterator j = resolvents.iterator();
				while (j.hasNext()) {
					FOPC.Node n = (FOPC.Node) j.next();

					if (n instanceof FOPC.TNode) {
						if (((FOPC.TNode) n)._bValue) {
							System.out
									.println("Should not get true TNode during res!");
						} else {
							if (ALL_PROOFS) {
								found_refutation = true;
							} else {
								_fQueryTime = GetElapsedTime() / 1000f;
								_nClausesGen = _hsActive.size() > _alClauses
										.size() ? _hsActive.size()
										- _alClauses.size() : _hsActive.size();

								return true; // Refutation found, don't need
												// all
								// proofs!
							}
						}
					} else if (n instanceof FOPC.ConnNode) {
						Clause new_cl = new Clause((FOPC.ConnNode) n);
						new_inferred.add(new_cl); // XXX ***
					} else {
						System.out.println("Illegal FOPC.Node durign res: "
								+ n.toFOLString());
					}
				}

			}
			_nResSteps += new_inferred.size();
			_llPassive.addAll(new_inferred);
			_hsActive.add(cur);
		}

		if (!found_refutation) {
			if (iterations >= MAX_ITERATIONS) {
				System.out.println("\n\nITERATION LIMIT REACHED.");
			} else {
				System.out.println("\n\nCLAUSE SET SATURATED.");
				Iterator i = _hsActive.iterator();
				while (i.hasNext()) {
					System.out.println("  -- " + i.next());
				}
			}
		}

		_fQueryTime = GetElapsedTime() / 1000f;
		_nClausesGen = _hsActive.size() - _alClauses.size();

		return found_refutation; // No refutation found
	}
	
	// Note that this requires a constraint on literal ordering
	// such that all negative literals are maximal.  See Clause
	// for implementation of this.
	public boolean canRefuteSOS() {

		_hsResolved = new HashSet();
		_hsFactored = new HashSet();
		_llPassive = null;
		_hsActive = null;
		_llSOS = new LinkedList();
		_hsUsable = new HashSet();
		_nResSteps = 0;
		_nClausesGen = 0;

		// First determine an elimination order (important if new predicates
		// added)
		setOrdering();

		// Initialize the active and passive lists
		// System.out.println("\n\n");
		//System.out.println("START: " + _alClauses);
		for (int i = 0; i < _alClauses.size(); i++) {
			FOPC.ConnNode c = (FOPC.ConnNode) _alClauses.get(i);
			if (isTautology(c)) {
				System.out.println("DISCARDING TAUTOLOGY: " + c);
				continue;
			}
			Clause cl = new Clause(c);
			// DEBUG
			// System.out.println("Adding clause: " + cl);
			
			// Add to correct list based on whether used in query
			if (i >= _nFirstQueryClause) {
				_llSOS.addLast(cl);
				_hsUsable.add(cl);
			} else {
				_hsUsable.add(cl);
			}

		}

		ResetTimer();
		Clause cur = null;

		// Take from passive first, then from active
		// Later should add a method for using age-weight ratio
		// to select active clause from age and weight queues!
		boolean found_refutation = false;

		// TODO: Make age and weight queues for selection.
		// System.out.println("\n\n");
		int iterations = 0;
		while (++iterations <= MAX_ITERATIONS && !_llSOS.isEmpty()
				&& (cur = (Clause) _llSOS.removeFirst()) != null) {

			if (PRINT_RES_STEP) {
				System.out.print("\n" + iterations 
						+ " SOS:" + _llSOS.size() + " U:" + _hsUsable.size() 
						+ ", examining clause: " + cur);
			} else if (iterations % 100 == 0) {
				System.out.println(iterations 
						+ " SOS:" + _llSOS.size() + " U:" + _hsUsable.size());
			}

			// Iterate through all active clauses, keeping track of resolvents
			Iterator i2 = _hsUsable.iterator(); // XXX ***
			Set resolvents = factorClause(cur);
			if (PRINT_RES_STEP) {
				System.out.println(", factors: " + resolvents);
			}
			HashSet new_inferred = new HashSet();

			while (iterations < MAX_ITERATIONS && i2.hasNext()) {
				Clause usable = (Clause) i2.next();
				_bPrintDebug = false;

				// DEBUG
				// if (cur.toString().indexOf("#R3") >= 0) {
				// System.out.println(" ++ resolve with clause: " + active);
				// _bPrintDebug = true;
				// }

				// Have cur and active, resolve and factor where possible
				Set res_set = resolveClauses(cur, usable);
				// DEBUG
				if (PRINT_RES_STEP && !res_set.isEmpty()) {
					System.out.println("\n" + cur + "     " + usable);
					System.out.println("------------------------------");
					Iterator i3 = res_set.iterator();
					while (i3.hasNext()) {
						System.out.println(((FOPC.Node) i3.next())
								.toFOLString()
								+ "\n");
					}
				}
				resolvents.addAll(res_set);

				Iterator j = resolvents.iterator();
				while (j.hasNext()) {
					FOPC.Node n = (FOPC.Node) j.next();

					if (n instanceof FOPC.TNode) {
						if (((FOPC.TNode) n)._bValue) {
							System.out
									.println("Should not get true TNode during res!");
						} else {
							if (ALL_PROOFS) {
								found_refutation = true;
							} else {
								_fQueryTime = GetElapsedTime() / 1000f;
								_nClausesGen = _llSOS.size(); // XXX Almost

								return true; // Refutation found, don't need
												// all
								// proofs!
							}
						}
					} else if (n instanceof FOPC.ConnNode) {
						Clause new_cl = new Clause((FOPC.ConnNode) n);
						new_inferred.add(new_cl); 
					} else {
						System.out.println("Illegal FOPC.Node durign res: "
								+ n.toFOLString());
					}
				}

			}
			_nResSteps += new_inferred.size();
			_hsUsable.addAll(new_inferred); // SOS may resolve with themselves, e.g. A <=> A
			_llSOS.addAll(new_inferred); // All resolvents added to SOS
			_llSOS.addLast(cur);
		}

		if (!found_refutation) {
			if (iterations >= MAX_ITERATIONS) {
				System.out.println("\n\nITERATION LIMIT REACHED.");
			} else {
				System.out.println("\n\nCLAUSE SET SATURATED.");
			}
			System.out.println("\nUsable:");
			Iterator i = _hsUsable.iterator();
			while (i.hasNext()) {
				System.out.println("  -- " + i.next());
			}
			System.out.println("\nSOS:");
			i = _llSOS.iterator();
			while (i.hasNext()) {
				System.out.println("  -- " + i.next());
			}
		}

		_fQueryTime = GetElapsedTime() / 1000f;
		_nClausesGen = _llSOS.size(); // XXX Almost

		return found_refutation; // No refutation found
	}
	
	// Find a theory refuting substitutions for the two PNodes
	// (if it exists)
	public HashMap findCSR(FOPC.PNode p1, FOPC.PNode p2) {

		// Unify them without comparing predicates (only arity)
		//System.out.println("FIND-CSR: " + p1.toFOLString() + ", " + p2.toFOLString());
		HashMap subst = FOPC.unify(p1, p2, true, false);
		if (subst == null)
			return null;

		// Check to see if syntactically complementary
		if (p1._bIsNegated == !p2._bIsNegated
				&& ((p1._nPredID != FOPC.PNode.INVALID && p1._nPredID == p2._nPredID) || 
					(p1._nPredID == FOPC.PNode.INVALID && p1._sPredName.equals(p2._sPredName)))) {
			if (PRINT_RES_STEP) {
				System.out.println("Compl: " + p1.toFOLString() + ", "
						+ p2.toFOLString());
			}
			return subst;
		}

		if (p1._nArity == 2 && p1._bIsNegated == !p2._bIsNegated) {

			if (p1._nPredID != FOPC.PNode.INVALID || p2._nPredID != FOPC.PNode.INVALID) {
				return null;
			}
			
			// Check to see if relation subsumption
			FOPC.PNode l1 = (p1._bIsNegated ? p2 : p1); // l1 is pos
			FOPC.PNode l2 = (p1._bIsNegated ? p1 : p2); // l2 is neg

			URI u1 = new URI(_kb._ir._context, l1._sPredName);
			URI u2 = new URI(_kb._ir._context, l2._sPredName);

			// If u1 => u2, then <u1, ~u2> must be disjoint.
			if (u1 != null && u2 != null
					&& _kb._tax._mmRParents.transClosure(u1).contains(u2)) {
				if (PRINT_RES_STEP) {
					System.out.println("Role SUB: " + p1.toFOLString() + ", "
							+ p2.toFOLString());
				}
				return subst;
			}

		} else if (p1._nArity == 1) {

			// Check to see if taxonomy subsumption
			URI u1 = new URI(_kb._ir._context, p1._sPredName);
			if (p1._bIsNegated) {
				u1 = (URI) _kb._ir._hmComplement.get(u1);
			}
			URI u2 = new URI(_kb._ir._context, p2._sPredName);
			URI not_u2 = null;
			if (p2._bIsNegated) {
				not_u2 = u2;
			} else {
				not_u2 = (URI) _kb._ir._hmComplement.get(u2);
			}

			// The complements of both u1 and u2 are indexed in the
			// taxonomy. Here, we just need to check that either,
			// u1 => ~u2 or equivalently u2 => ~u1. In either of these
			// cases <u1, ~u2> must be disjoint.
			// System.out.println(" -- testing concept sub: " + u1 + ", " +
			// not_u2);
			if (u1 != null && u2 != null
					&& _kb._tax._mmParents.transClosure(u1).contains(not_u2)) {
				if (PRINT_RES_STEP) {
					System.out.println("Concept SUB: " + p1.toFOLString()
							+ ", " + p2.toFOLString());
				}
				return subst;
			}

		}

		return null;
	}

	// Factor a clause (using syntactic equivalence for now!)
	public Set factorClause(Clause clause) {

		// System.out.println("Attempting to factor: " + clause);

		HashSet ret_set = new HashSet();

		FOPC.ConnNode c = clause._clause;
		if (_hsFactored.contains(c))
			return ret_set;
		_hsFactored.add(c);
		Iterator c1_iter = clause._tsMaxLit.iterator();
		while (c1_iter.hasNext()) {

			int pos1 = ((Integer) c1_iter.next()).intValue();
			FOPC.PNode p1 = (FOPC.PNode) c._alSubNodes.get(pos1);
			Iterator c2_iter = clause._tsMaxLit.iterator();
			while (c2_iter.hasNext()) {

				int pos2 = ((Integer) c2_iter.next()).intValue();
				if (pos1 == pos2)
					continue;
				FOPC.PNode p2 = (FOPC.PNode) c._alSubNodes.get(pos2);

				// Are p1 and p2 syntactically equivalent and same polarity?
				HashMap theta = null;
				if (p1._bIsNegated == p2._bIsNegated
						&& (theta = FOPC.unify(p1, p2, false, true)) != null) {

					// Go ahead with factoring step
					FOPC.ConnNode f = (FOPC.ConnNode) c.copy();
					f.substitute(theta);
					f._alSubNodes.remove(pos2);
					standardizeClause(f);
					ret_set.add(f);
				}
			}
		}

		return ret_set;
	}

	// Resolve clauses over predicate and return new list (minus predicate!)
	public Set resolveClauses(Clause cl_1, Clause cl_2) {

		// System.out.println("Attempting to resolve: " + cl_1 + ", " + cl_2);

		HashSet return_set = new HashSet();
		FOPC.ConnNode c1 = cl_1._clause;
		FOPC.ConnNode c2 = cl_2._clause;
		FOPC.NodePair p = new FOPC.NodePair(c1, c2);

		// Have we performed this resolution already?
		if (_hsResolved.contains(p))
			return return_set;

		_hsResolved.add(p);

		// Determine all possible resolutions of c1, c2
		Iterator c1_iter = cl_1._tsMaxLit.iterator();
		while (c1_iter.hasNext()) {

			int pos1 = ((Integer) c1_iter.next()).intValue();
			FOPC.PNode p1 = (FOPC.PNode) c1._alSubNodes.get(pos1);
			Iterator c2_iter = cl_2._tsMaxLit.iterator();
			while (c2_iter.hasNext()) {

				int pos2 = ((Integer) c2_iter.next()).intValue();
				FOPC.PNode p2 = (FOPC.PNode) c2._alSubNodes.get(pos2);

				// These two match, so try to resolve
				if (PRINT_RESOLUTIONS) {
					System.out.print("Resolving " + c1.toFOLString() + " ["
							+ pos1 + "] " + ", " + c2.toFOLString() + " ["
							+ pos2 + "] ");
				}
				FOPC.Node res = resolveClauses(c1, pos1, c2, pos2);

				// Add to correct lists
				if (res == null) {
					if (PRINT_RESOLUTIONS) {
						System.out.println(" -> Could not unify!\n");
					}
					continue;
				}
				// Keep track of proof
				if (MAINTAIN_PROOFS
						&& (!(res instanceof FOPC.TNode) || !((FOPC.TNode) res)._bValue)) {
					addProofStep(c1, c2, res);
				}

				if (PRINT_RESOLUTIONS) {
					System.out.println(" -> " + res.toFOLString() + "\n");
				}

				if (res instanceof FOPC.ConnNode) {
					return_set.add(res);
				} else {
					if (!((FOPC.TNode) res)._bValue) {
						return_set.add(res);
						if (!ALL_PROOFS) {
							return return_set; // Exit early if only one proof
						}
					} // else trye TNode
				}
			}
		}

		return return_set;
	}

	// Find resolvent of two clauses by unifying and eliminating the
	// respective positions (pos1, pos2 passed as arguments).
	// Makes sure duplicates are removed from resolved clause.
	// Return null if no unification possible.
	// Note: unify will standardize apart!
	public FOPC.Node resolveClauses(FOPC.ConnNode c1, int pos1,
			FOPC.ConnNode c2, int pos2) {

		if (PRINT_RES_STEP && _bPrintDebug) {
			System.out.println("     -- RESOLVE LITERALS "
					+ c1._alSubNodes.get(pos1) + ", "
					+ c2._alSubNodes.get(pos2) + " ");
		}

		// Use positive ordered resolution only for the non-DL portions
		// of the clauses (if used on DL portions, it would be incomplete).
		if (containsNegativeResidue(c1, pos1) &&
			containsNegativeResidue(c2, pos2)) {
			//System.out.println("Discarding resolution on " + c1 + ":" + pos1 + ", " + c2 + ":" + pos2);
			return null;
		}
		
		// Don't resolve a clause with itself
		if (c1.equals(c2)) {
			// System.out.println("*SQUASH*");
			return null;
		}
		// System.out.println("*KEEP*");

		// Copy clauses so unification does not corrupt other versions
		c1 = (FOPC.ConnNode) c1.copy();
		c2 = (FOPC.ConnNode) c2.copy();
		FOPC.PNode p1 = (FOPC.PNode) c1._alSubNodes.get(pos1);
		FOPC.PNode p2 = (FOPC.PNode) c2._alSubNodes.get(pos2);

		// Always standardize apart clauses before unification (reset var count
		// first)
		FOPC.TVar._nVarCount = 0;
		HashMap theta1 = FOPC.standardizeApartClause(c1);
		HashMap theta2 = FOPC.standardizeApartClause(c2);
		c1.substitute(theta1);
		c2.substitute(theta2);

		// Unify on the specified predicates
		HashMap theta = findCSR(p1, p2);
		if (theta == null) {
			return null;
		}

		// System.out.println("Before sub: " + c1.toFOLString() + ", " +
		// c2.toFOLString());
		c1.substitute(theta);
		c2.substitute(theta);
		// System.out.println("After sub: " + c1.toFOLString() + ", " +
		// c2.toFOLString());

		// Make new unified clause without resolved PNodes
		if (c1._alSubNodes.size() == 1 && c2._alSubNodes.size() == 1) {

			// ///////////////////////////////////////////////////////////////////////
			// Add query bindings if found
			// ///////////////////////////////////////////////////////////////////////
			if (_bs != null
					&& ((FOPC.PNode) c1._alSubNodes.get(0))._sPredName == QUERY) {

				//System.out.println("Bindings solution: " + ((FOPC.PNode) c1._alSubNodes.get(0)));
				
				// Add bindings to binding set if we are querying for free vars
				Iterator ans_it = ((FOPC.PNode) c1._alSubNodes.get(0))._alTermBinding
						.iterator();
				Iterator bind_it = _alQueryBindings.iterator();

				// The following determines whether to actually add this answer
				boolean collect = true;
				if (COLLECT_ONLY_CONSTANTS) {
					while (ans_it.hasNext()) {
						FOPC.Term ans_term = (FOPC.Term) ans_it.next();
						if (ans_term instanceof FOPC.TVar) {
							collect = false;
						} else if (ans_term instanceof FOPC.TFunction) {
							// May want to keep generic functions
							if (((FOPC.TFunction) ans_term)._nArity > 0) {
								collect = false;
							}
						}
					}
					// Reset iterator
					ans_it = ((FOPC.PNode) c1._alSubNodes.get(0))._alTermBinding
							.iterator();
				}

				if (collect) {
					int entry = _bs.makeNewBindingEntry();
					while (ans_it.hasNext()) {
						FOPC.TVar bind_var = (FOPC.TVar) bind_it.next();
						FOPC.Term ans_term = (FOPC.Term) ans_it.next();
						_bs.addBinding(entry, bind_var.toString(), ans_term
								.toString());
					}
				}
			}
			// ///////////////////////////////////////////////////////////////////////

			return new FOPC.TNode(false); // Must resolve to empty set!
		}

		HashSet already_added = new HashSet(); // Check for duplicates and
		// remove!
		FOPC.ConnNode ret = new FOPC.ConnNode(FOPC.ConnNode.OR);

		// Add nodes from clause 1
		int sz1 = c1._alSubNodes.size();
		for (int pc1 = 0; pc1 < sz1; pc1++) {
			FOPC.Node to_add = (FOPC.Node) c1._alSubNodes.get(pc1);
			if (pc1 != pos1 && !already_added.contains(to_add)) {
				ret.addSubNode(to_add);
				already_added.add(to_add);
			}
		}

		// Add nodes from clause 2
		int sz2 = c2._alSubNodes.size();
		for (int pc2 = 0; pc2 < sz2; pc2++) {
			FOPC.Node to_add = (FOPC.Node) c2._alSubNodes.get(pc2);
			if (pc2 != pos2 && !already_added.contains(to_add)) {
				ret.addSubNode(to_add);
				already_added.add(to_add);
			}
		}

		// Standardize apart when adding back -- makes redundancy detection
		// easier
		if (isTautology(ret)) {
			return null;
		}
		standardizeClause(ret);

		return ret;
	}

	// XXX Syntactic tautology check
	public boolean isTautology(FOPC.Node n) {

		if (n instanceof FOPC.TNode) {
			return ((FOPC.TNode) n)._bValue;
		} else {
			FOPC.ConnNode c = (FOPC.ConnNode) n;
			for (int i = 0; i < c._alSubNodes.size(); i++) {
				FOPC.PNode p1 = (FOPC.PNode) c._alSubNodes.get(i);
				for (int j = i + 1; j < c._alSubNodes.size(); j++) {
					FOPC.PNode p2 = (FOPC.PNode) c._alSubNodes.get(j);
					if (p1._bIsNegated == !p2._bIsNegated
							&& p1._nArity == p2._nArity
							&& p1._nPredID == p2._nPredID 
							&& (p1._nPredID != FOPC.PNode.INVALID 
							    || p1._sPredName.equals(p2._sPredName))) {

						// Check for equivlance of term bindings
						boolean equiv = true;
						for (int k = 0; equiv && k < p1._alTermBinding.size(); k++) {
							equiv = p1._alTermBinding.get(k).equals(
									p2._alTermBinding.get(k));
						}
						if (equiv) {
							// System.out.println(" ** DISCARDING TAUTOLOGY: " +
							// n);
							return true; // Tautological node pair found
						}
					}
				}
			}
		}

		return false;
	}

	// XXX Reorder PNodes

	public TreeMap _m = new TreeMap();

	public void standardizeClause(FOPC.ConnNode c) {
		int k = 0;
		ArrayList l = c._alSubNodes;
		_m.clear();
		for (int i = 0; i < l.size(); i++) {
			FOPC.PNode p = (FOPC.PNode) l.get(i);
			String cur_str = p._sPredName + "_" + p._nArity + "_"
					+ (p._bIsNegated ? "T" : "F") + k;
			if (_m.containsKey(cur_str)) {
				cur_str = p._sPredName + "_" + p._nArity + "_"
						+ (p._bIsNegated ? "T" : "F") + ++k;
			}
			_m.put(cur_str, p);
		}
		l.clear();
		Iterator j = _m.entrySet().iterator();
		while (j.hasNext()) {
			Map.Entry me = (Map.Entry) j.next();
			l.add(me.getValue());
		}

		// Now that nodes are reordered, standard var names
		FOPC.TVar._nVarCount = 0;
		HashMap subst = FOPC.standardizeApartClause(c);
		c.substitute(subst);
	}

	public void addFormula(FOPC.Node n) {

		// DNF conversion to push down quantifiers is pointless here
		// since we'll get rid of EXISTS and CNF(DNF) will yield
		// an exponential blowup with simplification.
		//System.out.println("ADD: " + n);
		boolean SAVED_ALLOW_DNF = FOPC.ALLOW_DNF;
		FOPC.ALLOW_DNF = false;

		// Simpify n and convert to NNF
		// System.out.println(n.toFOLString());
		n = n.copy(); // Following functions may modify original!
		n = FOPC.simplify(n);
		n = FOPC.skolemize(n);
		n = FOPC.convertCNF(n);

		// Break apart top-level, standardize apart free vars, and add to clause
		// list
		if ((n instanceof FOPC.ConnNode)
				&& ((FOPC.ConnNode) n)._nType == FOPC.ConnNode.AND) {

			// Go through all subclauses
			Iterator i = ((FOPC.ConnNode) n)._alSubNodes.iterator();
			while (i.hasNext()) {
				FOPC.Node sn = (FOPC.Node) i.next();

				// Reality check!
				if (!(((sn instanceof FOPC.ConnNode) && ((FOPC.ConnNode) sn)._nType == FOPC.ConnNode.OR) || (sn instanceof FOPC.PNode))) {

					System.out.println("CNF conversion was bad!");
					// System.exit(1);
				}

				if (sn instanceof FOPC.PNode) {
					FOPC.ConnNode cn = new FOPC.ConnNode(FOPC.ConnNode.OR);
					cn.addSubNode(sn);
					if (PRINT_CNF) {
						System.out.println("CNF: " + cn);
					}
					standardizeClause(cn);
					_alClauses.add(cn);
				} else {
					if (PRINT_CNF) {
						System.out.println("CNF: " + sn);
					}
					standardizeClause((FOPC.ConnNode) sn);
					_alClauses.add((FOPC.ConnNode) sn);
				}
			}

		} else {
			if (!(((n instanceof FOPC.ConnNode) && ((FOPC.ConnNode) n)._nType == FOPC.ConnNode.OR) || (n instanceof FOPC.PNode))) {

				System.out.println("CNF conversion was bad!");
				// System.exit(1);
			}

			if (n instanceof FOPC.PNode) {
				FOPC.ConnNode cn = new FOPC.ConnNode(FOPC.ConnNode.OR);
				cn.addSubNode(n);
				if (PRINT_CNF) {
					System.out.println("CNF: " + cn);
				}
				standardizeClause(cn);
				_alClauses.add(cn);
			} else {
				if (PRINT_CNF) {
					System.out.println("CNF: " + n);
				}
				standardizeClause((FOPC.ConnNode) n);
				_alClauses.add(n);
			}
		}

		// Restore DNF conversion
		FOPC.ALLOW_DNF = SAVED_ALLOW_DNF;
	}

	public String toString() {
		return printClauses(_hmPred2Clause);
	}

	public String printClauses(HashMap hm) {
		StringBuffer sb = new StringBuffer();
		Iterator i = hm.entrySet().iterator();
		while (i.hasNext()) {
			Map.Entry me = (Map.Entry) i.next();
			sb.append("**" + me.getKey() + "**:\n" + me.getValue() + "\n");
		}
		return sb.toString();
	}

	public class Clause implements Comparable {

		public FOPC.ConnNode _clause;

		public TreeSet _tsMaxLit;

		public Clause(FOPC.ConnNode c) {
			_clause = c;
			_tsMaxLit = new TreeSet();

			boolean all_pos = true;
			for (int i = 0; all_pos && i < _clause._alSubNodes.size(); i++) {
				all_pos = !((FOPC.PNode) _clause._alSubNodes.get(i))._bIsNegated;
			}
			
			// Determine which elements of this clause are maximal literals
			// A literal s is maximal if compared to all other literals t_i,
			// it is not the case that CompareLiteral(s, t_i) < -1. (If this
			// were true then s would be dominated by t_i in the ordering.)
			for (int i = 0; i < _clause._alSubNodes.size(); i++) {
				
				FOPC.PNode s = (FOPC.PNode) _clause._alSubNodes.get(i);
				
				boolean maximal = true;
				if (USE_SOS && !all_pos) {
					// If SOS clause is not all positive then all literals selected
					// TODO: Refinement for excluding DL literals from all_pos?
					maximal = true;
				} else {				
					for (int j = 0; maximal && j < _clause._alSubNodes.size(); j++) {
						if (i == j)
							continue;
						FOPC.PNode t = (FOPC.PNode) _clause._alSubNodes.get(j);
	
						maximal = compareLiterals(s, t) >= -1;
					}
				}

				if (maximal) {
					_tsMaxLit.add(new Integer(i));
				}
			}
		}

		public int hashCode() {
			return _clause.hashCode();
		}

		public boolean equals(Object o) {
			Clause c = (Clause) o;
			return c._clause.equals(this._clause);
		}

		public int compareTo(Object o) {
			Clause c = (Clause)o;
			int this_sz = this._clause._alSubNodes.size();
			int c_sz    = c._clause._alSubNodes.size();
			
			// Shorter clause first
			if (this_sz < c_sz) {
				return -1;
			} else if (this_sz > c_sz) {
				return 1;
			}
			
			// If clauses equal, must show this!
			if (this.equals(c)) {
				return 0;
			} else {
				
				ArrayList this_terms = new ArrayList();
				int this_max = 0; // max single term
				for (int j = 0; j < this._clause._alSubNodes.size(); j++) {
					FOPC.PNode p = (FOPC.PNode)this._clause._alSubNodes.get(j);
					for (int i = 0; i < p._alTermBinding.size(); i++) {
						List t = ((FOPC.Term) p._alTermBinding.get(i))
									.getListOfTerms();
						this_terms.addAll(t);
						if (t.size() > this_max) this_max = t.size();
					}
				}

				ArrayList c_terms = new ArrayList();
				int c_max = 0; // max single term
				for (int j = 0; j < c._clause._alSubNodes.size(); j++) {
					FOPC.PNode p = (FOPC.PNode)c._clause._alSubNodes.get(j);
					for (int i = 0; i < p._alTermBinding.size(); i++) {
						List t = ((FOPC.Term) p._alTermBinding.get(i))
									.getListOfTerms();
						c_terms.addAll(t);
						if (t.size() > c_max) c_max = t.size();
					}
				}

				// Total term size (incorporates max because of list)
				if (this_terms.size() < c_terms.size()) {
					return -1;
				} else if (c_terms.size() < this_terms.size()) {
					return 1;
				}

				// Max single term size
				if (this_max < c_max) {
					return -1;
				} else if (c_max < this_max) {
					return 1;
				}
				
				// Lexicographic comparison
				String this_str = this._clause.toFOLString();
				String c_str    = this._clause.toFOLString();
				return this_str.compareTo(c_str);
			}
		}
		
		public TreeSet getMaxLiterals() {
			return _tsMaxLit;
		}

		public String toString() {
			StringBuffer sb = new StringBuffer();
			for (int i = 0; i < _clause._alSubNodes.size(); i++) {
				FOPC.PNode s = (FOPC.PNode) _clause._alSubNodes.get(i);
				boolean maximal = _tsMaxLit.contains(new Integer(i));
				if (maximal) {
					sb.append(((i > 0) ? " | " : "") + "[ " + s.toFOLString()
							+ " ]");
				} else {
					sb.append(((i > 0) ? " | " : "") + s.toFOLString());
				}
			}
			return sb.toString();
		}
	}

	public void printProofs(FOPC.Node n, boolean display) {
		_nProofSteps = 0;
		printProofs(n, 0, new HashSet(), display);
	}

	// sz is equivalent to number of resolution step recursions
	public void printProofs(FOPC.Node n, int sz, HashSet shown, boolean display) {

		if (sz > _nProofSteps) {
			_nProofSteps = sz;
		}

		if (display) indent(sz);
		sz++;
		if (display) System.out.print("- " + n.toFOLString());
		if (shown.contains(n)) {
			if (display) System.out.println(" <- [ALREADY SHOWN]");
			return;
		}
		shown.add(n);
		Set s = getNodeProofs(n, false);
		if (s == null) {
			if (display) System.out.println(" <- [GIVEN]");
		} else {
			Iterator i = s.iterator();
			while (i.hasNext()) {
				FOPC.NodePair np = (FOPC.NodePair) i.next();
				if (display) System.out.println(" <- ");
				printProofs(np._n1, sz, shown, display);
				printProofs(np._n2, sz, shown, display);
				if (PRINT_FIRST_PROOF_ONLY) {
					break;
				}
			}
		}
	}

	public void addProofStep(FOPC.Node n1, FOPC.Node n2, FOPC.Node res) {
		Set s = getNodeProofs(res, true);
		s.add(new FOPC.NodePair(n1, n2));
	}

	public Set getNodeProofs(FOPC.Node res, boolean create) {
		Set s = (Set) _hmProofs.get(res);
		if (s == null && create) {
			s = new HashSet();
			_hmProofs.put(res, s);
		}
		return s;
	}

	public void indent(int l) {
		for (int i = 0; i < l; i++) {
			System.out.print("   ");
		}
	}

	public void ResetTimer() {
		_lTime = System.currentTimeMillis();
	}

	// Get the elapsed time since resetting the timer
	public long GetElapsedTime() {
		return System.currentTimeMillis() - _lTime;
	}

	public class ClausePriorityQueue {
		
		public TreeSet ts;
		
		public ClausePriorityQueue() {
			ts = new TreeSet();
		}
		
		public void insert(Clause c) {
			ts.add(c);
		}
		
		public boolean remove(Clause c) {
			return ts.remove(c);
		}
			
		public Clause dequeue() {
			if (ts.isEmpty()) {
				return null;
			}
			
			Clause c = (Clause)ts.iterator().next();
			ts.remove(c);
			return c;
		}
		
		public void clear() {
			ts.clear();
		}
		
		public boolean isEmpty() {
			return ts.isEmpty();
		}
		
		public int size() {
			return ts.size();
		}
	}
	
	// /////////////////////////////////////////////////////////////////////////
	// Inference Routine
	// /////////////////////////////////////////////////////////////////////////

	// /////////////////////////////////////////////////////////////////////////
	// Test Routine
	// /////////////////////////////////////////////////////////////////////////

}
